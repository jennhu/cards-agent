\documentclass[11pt]{article}

% BASIC PACKAGES
\usepackage{latexsym}
\usepackage{amssymb,amsmath,pgf,graphicx}
\usepackage{amsthm}
\usepackage{thmtools}

% FONTS AND SPACING
\usepackage{amsfonts}
\usepackage{tgpagella} % font
\usepackage[margin=0.8in]{geometry}
\usepackage{setspace}
%\onehalfspacing % or
\doublespacing

% SPECIAL FORMATTING
\usepackage{fancyhdr}
\usepackage[T1]{tipa} % for ipa
\usepackage{color}
\usepackage{tabulary}
\usepackage{mdframed}
\usepackage{algpseudocode}
\usepackage{tikz}

% MATHEMATICAL COMMANDS
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\bvec}[1]{\vec{\mathbf{#1}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\overlap}{\textsc{Overlap}}
\newcommand{\lkhd}{\textsc{Likelihood}}
\newcommand{\goodaction}{\textsc{GoodAction}}

% DEFINITIONS AND THEOREMS
\newtheorem{theorem}{Theorem}
\declaretheorem[style=definition,qed=$\blacksquare$]{definition}

\pagestyle{fancy}
\fancyhf{}
\rhead{\today}
\lhead{Jennifer Hu}
\cfoot{\thepage}

\begin{document}

\begin{center}
\Large{Finding optimal goals in card games with uncertainty}
\end{center}

\section{Introduction}

Given the history of the card game, what is the optimal goal to pursue? By building a bot that can answer this question at every step of the game, we can evaluate human players' performance, rationality, and goal-specific language in a systematic way.

In a standard 52-card deck, there are 52 possible straight flushes of 6 cards. Each of these 6-card straight flushes is called a goal, and we give weights to each of them in the vector $\bvec{w} \in \R^{52}$, which is updated every time we learn new information about the game. Before any cards are dealt, every goal is equally (un)optimal, and $\bvec{w} := \bvec{0}$. The game history contains information about the number of cards reshuffled at each round and the cards we have already seen.

\section{The Features}

When expert humans play the game, they tend to evaluate goals based on three features: (1) how many accessible cards (i.e. in the hands and table) coincide with the goal, (2) how likely it is to obtain the goal based on which cards might have already been discarded, and (3) whether or not they can take an action that brings them closer to the goal during their turn. I call these properties $\overlap$, $\lkhd$, and $\goodaction$, respectively.

\subsection{$\overlap$}

$\overlap$ measures how many cards in common a certain goal $G$ has with a set of cards $C$. Formally, $\overlap$ is defined as the following:
\begin{equation}
\overlap(G,C) = |G \cap C|
\label{eq:overlap} \end{equation}
$\overlap$ takes on values in the interval $[0,6]$. It is independent of game history.

Another way of thinking of $\overlap$ is that $6-\overlap(G,C)$ represents the number of cards ``away'' $C$ is from goal $G$.

\subsection{$\lkhd$}

$\lkhd$ measures how likely it is that a certain goal $G$ can be obtained given all previous history $H$. $\lkhd$ is defined as the following:
\begin{align}
  \mathcal{L}(G,H) &= \sum_{g \in G} (1 - P(g \text{ has been discarded}|H))
\end{align}

Given a card $g$ and history $H$, we can find $P(g \text{ has been discarded}|H)$ in the following way. First, we know $H$ contains information about $r_i$, the number of cards reshuffled at round $i$ for all $i$, as well as which cards we have and have not seen. Let $s$ be the index of the round that $g$ was last seen, let $n$ be the number of rounds that have occurred between $s$ and now, and let $D_i$ be the size of the deck at round $i$. Then we have:
\begin{equation}
P(g \text{ has been discarded}|H) = \begin{cases}
  0 & g \in \text{ hands or table, or unseen} \\
  \frac{4-r_s}{4-r_s+r_s\prod_{j=1}^n(1-4/{D_{s+j}})} & \text{o.w.}
\end{cases}
\label{eq:p-discarded} \end{equation}

\begin{proof}
  It is easy to see that a card $g$ cannot have been discarded if it is currently in the hands, on the table, or unseen (in the deck). Now, consider the case where $g$ has not been seen for $n$ rounds (since round $s$). Let $F$ be the event that $g$ was discarded at round $s$, and let $U$ be the event that $g$ hasn't been seen for $n$ rounds. By Bayes' Rule, we have:
\begin{align}
  P(F|U) &= \frac{P(U|F)P(F)}{P(U)} \\
  &= \frac{(1)\left(\frac{4-r_s}{4}\right)}{P(U|F)P(F) + P(U|F^c)P(F^c)} \\
  &= \frac{(4-r_s)/4}{(4-r_s)/4 + (r_s/4)P(U|F^c)}
\end{align}

Let's look at $P(U|F^c)$. Since we are conditioning on $F^c$, we know that $g$ was reshuffled in round $s$, and the size of the deck was updated to $D_s$. The probability of not drawing $g$ in the next round $s+1$ was
\begin{equation}
  \frac{{D_{s+1}-1 \choose 4}}{{D_{s+1} \choose 4}} = \frac{D_{s+1}-4}{D_{s+1}},
\end{equation} and this pattern continued for every subsequent round until the current round $s+n$. This gives us
\begin{equation}
  P(U|F^c) = \prod_{j=1}^n \frac{D_{s+j} - 4}{D_{s+j}}.
\end{equation}

Using this result and simplifying, we finally obtain
\begin{equation}
  P(F|U) = \frac{4-r_s}{4-r_s+r_s\prod_{j=1}^n(1-\frac{4}{{D_{s+j}}})}.
\end{equation}
\end{proof}

See (\ref{eq:decksize}) for an expression for the deck size at each round.

\subsubsection{Deck Size}

At round $0$, the deck has 52 cards. The size of the deck at round $k > 0$ is given by
\begin{equation}
  D_k = 46 + \sum_{i=1}^{k-1} r_i - 4k.
\label{eq:decksize} \end{equation}

\begin{proof}
  In the base case, $k=1$ yields $46+0-4(1) = 42$, which is correct since we deal 10 cards from the original 52 in the first round and no cards are reshuffled. Suppose now that (\ref{eq:decksize}) is true for $k=n$. Then we have:
    \begin{align}
      D_{n+1} &= D_n + r_n - 4 \\
      &= (46 + \sum_{i=1}^{n-1} r_i - 4n) + r_n - 4 \\
      &= 46 + \sum_{i=1}^n r_i - 4(n+1)
    \end{align}
  By induction, (\ref{eq:decksize}) holds for all values of $k$.
\end{proof}

\subsection{$\goodaction$}

With $\overlap$ and $\lkhd$, the goal agent gives a good sense of which goals are optimal at each step, but it treats both players' hands as a single object and does not reason about goals using the knowledge that each player must alternate turns. $\goodaction$ incorporates turn-taking into the goal evaluation model. As expert humans, we've found this to be the most challenging part of the game and the primary impetus for goal-changing.

I define $\goodaction$ as the following:
\begin{equation}
  \goodaction(G) = T_1 \cdot A_1(G) + T_2 \cdot A_2(G),
\label{eq:goodaction} \end{equation}
where $T_i$ and $A_i$ are defined as
\begin{align}
  T_i &= \begin{cases}
    1 & \text{it's Player $i$'s turn} \\
    0 & \text{o.w.}
  \end{cases} \\
  A_i(G) &= \begin{cases}
    1 & \exists \text{ an action Player $i$ can take that will bring them closer to $G$} \\
    0 & \text{o.w.}
  \end{cases}
\end{align}

An ``action'' is defined as a swap of 1, 2, or 3 cards between a player's hand and the table. Getting ``closer'' to a goal $G$ is equivalent to increasing the $\overlap$. Note that $\goodaction(G)$ implicitly depends on the game history $H$ in order to access information about whose turn it is and the players' hands.

\section{Evaluating Goals}

Every time the history $H$ is updated and we observe a new set of cards $C$, the weights $w_i$ for each goal $G_i$ are updated with a linear combination of the two metrics:
\begin{equation}
  w_i := \alpha_0 \cdot \overlap(G_i,C) + \alpha_1 \cdot \lkhd(G_i,H) + \alpha_2 \cdot \goodaction(G_i)
\label{eq:w-update} \end{equation}

The values of $\{\alpha\}$ depend on how ``rational'' we want the goal agent to be. A more human-like player might weigh $\overlap$ over $\lkhd$, for example, since it tends to consider only the visible cards and does not calculate probabilities based on game history. Expert humans might also be more likely to put high weight on $\goodaction$.

\section{Issues and Next Steps}

This goal evaluation model is rather simple, but its output at each step makes sense from a human assessment.

The first issue is what values to choose for $\{\alpha\}$. Since we don't have a ``ground truth'' optimal goal baseline (which is why we're making this one!), it's difficult to evaluate the model's output in a rigorous way. Different values of $\{\alpha\}$ do noticeably affect the output, and it seems counterproductive to rely on human judgment to compare the outputs.

Next, we face the issue of computational complexity.

\end{document}
