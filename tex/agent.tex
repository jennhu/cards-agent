\documentclass[11pt]{article}

% BASIC PACKAGES
\usepackage{latexsym}
\usepackage{amssymb,amsmath,pgf,graphicx}
\usepackage{amsthm}
\usepackage{thmtools}

% FONTS AND SPACING
\usepackage{amsfonts}
\usepackage{tgpagella} % font
\usepackage[margin=0.8in]{geometry}
\usepackage{setspace}
%\onehalfspacing % or
\doublespacing

% SPECIAL FORMATTING
\usepackage{fancyhdr}
\usepackage[T1]{tipa} % for ipa
\usepackage{color}
\usepackage{tabulary}
\usepackage{mdframed}
\usepackage{algpseudocode}
\usepackage{tikz}

% MATHEMATICAL COMMANDS
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\bvec}[1]{\vec{\mathbf{#1}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\overlap}{\textsc{Overlap}}
\newcommand{\lkhd}{\textsc{Likelihood}}
\newcommand{\goodaction}{\textsc{GoodAction}}

% DEFINITIONS, THEOREMS, & CLAIMS
\newtheorem{theorem}{Theorem}
\declaretheorem[style=definition,qed=$\blacksquare$]{definition}
\newenvironment{claim}[1]{\par\noindent\textit{Claim.}\space#1}{\hfill $\blacksquare$}

\pagestyle{fancy}
\fancyhf{}
\rhead{\today}
\lhead{Jennifer Hu}
\cfoot{\thepage}

\begin{document}

\begin{center}
\Large{Finding optimal goals in card games with uncertainty}
\end{center}

\section{Introduction}

Given the history of the card game, what is the optimal goal to pursue? By building a bot that can answer this question at every step of the game, we can evaluate human players' performance, rationality, and goal-oriented language in a systematic way.

In a standard 52-card deck, there are 52 possible straight flushes of 6 cards. Each of these 6-card straight flushes is called a goal, and we give weights to each of them in the vector $\bvec{w} \in \R^{52}$, which is updated every time we learn new information about the game. Before any cards are dealt, every goal is equally (un)optimal, and $\bvec{w} := \bvec{0}$. The game history contains information about the number of cards reshuffled at each round and the cards we have already seen.

\section{The Features}

When expert humans play the game, they tend to evaluate goals based on three features: (1) how many visible cards (i.e. in the hands and table) coincide with the goal, (2) how likely it is to obtain the goal based on which cards might have already been discarded, and (3) whether or not they can take an action that brings them closer to the goal during their turn. I call these features $\overlap$, $\lkhd$, and $\goodaction$, respectively.

\subsection{$\overlap$}

$\overlap$ measures how many cards in common a certain goal $G$ has with a set of cards $C$. Formally, $\overlap$ is defined as the following:
\begin{equation}
\overlap(G,C) = |G \cap C|
\label{eq:overlap} \end{equation}
$\overlap$ takes on values in the interval $[0,6]$. It is independent of game history.

Another way of thinking of $\overlap$ is that $6-\overlap(G,C)$ represents the number of cards ``away'' $C$ is from goal $G$.

\subsection{$\lkhd$}

$\lkhd$ measures how likely it is that a certain goal $G$ can be obtained given all previous history $H$. $\lkhd$ is defined as the following:
\begin{align}
  \mathcal{L}(G,H) &= \sum_{g \in G} \log(1 - P(g \text{ has been discarded}|H))
\end{align}

Given a card $g$ and history $H$, we can find $P(g \text{ has been discarded}|H)$ in the following way. First, we know $H$ contains information about $r_i$, the number of cards reshuffled at round $i$ for all $i$, as well as which cards we have and have not seen. Let $s$ be the index of the round that $g$ was last seen, let $n$ be the number of rounds that have occurred between $s$ and now, and let $D_i$ be the size of the deck at round $i$.

\begin{claim}
\begin{equation}
P(g \text{ has been discarded}|H) = \begin{cases}
  0 & g \in \text{ hands or table, or unseen} \\
  \frac{4-r_s}{4-r_s+r_s\prod_{j=1}^n(1-4/{D_{s+j}})} & \text{o.w.}
\end{cases}
\label{eq:p-discarded} \end{equation}
\end{claim}

\begin{proof}
  It is easy to see that a card $g$ cannot have been discarded if it is currently in the hands, on the table, or unseen (in the deck). Now, consider the case where $g$ has not been seen for $n$ rounds (i.e. since round $s$). Let $F$ be the event that $g$ was discarded at round $s$, and let $U$ be the event that $g$ hasn't been seen for $n$ rounds. By Bayes' Rule, we have:
\begin{align}
  P(F|U) &= \frac{P(U|F)P(F)}{P(U)} \\
  &= \frac{(1)\left(\frac{4-r_s}{4}\right)}{P(U|F)P(F) + P(U|F^c)P(F^c)} \\
  &= \frac{(4-r_s)/4}{(4-r_s)/4 + (r_s/4)P(U|F^c)}
\end{align}

Let's look at $P(U|F^c)$. Since we are conditioning on $F^c$, we know that $g$ was reshuffled in round $s$, and the size of the deck was updated to $D_s$. The probability of not drawing $g$ in the next round $s+1$ was
\begin{equation}
  \frac{{D_{s+1}-1 \choose 4}}{{D_{s+1} \choose 4}} = \frac{D_{s+1}-4}{D_{s+1}},
\end{equation} and this pattern continued for every subsequent round until the current round $s+n$. This gives us
\begin{equation}
  P(U|F^c) = \prod_{j=1}^n \frac{D_{s+j} - 4}{D_{s+j}}.
\end{equation}

Using this result and simplifying, we finally obtain
\begin{equation}
  P(F|U) = \frac{4-r_s}{4-r_s+r_s\prod_{j=1}^n(1-\frac{4}{{D_{s+j}}})}.
\end{equation}
\end{proof}

See (\ref{eq:decksize}) for an expression for the deck size at each round.

\subsubsection{Deck Size}

It is useful to keep track of the size of the deck at each round $k$ in order to calculate the relevant probabilities. Before any cards are dealt (i.e. round $k=0$), the deck has 52 cards.

\begin{claim}
The size of the deck at round $k > 0$ is given by
\begin{equation}
  D_k = 46 + \sum_{i=1}^{k-1} r_i - 4k.
\label{eq:decksize} \end{equation}
\end{claim}

\begin{proof}
  In the base case, $k=1$ yields $46+0-4(1) = 42$, which is correct since we deal 10 cards from the original 52 in the first round and no cards are reshuffled. Suppose now that (\ref{eq:decksize}) is true for $k=n$. Then we have:
    \begin{align}
      D_{n+1} &= D_n + r_n - 4 \\
      &= (46 + \sum_{i=1}^{n-1} r_i - 4n) + r_n - 4 \\
      &= 46 + \sum_{i=1}^n r_i - 4(n+1)
    \end{align}
  By induction, (\ref{eq:decksize}) holds for all values of $k$.
\end{proof}

\subsection{$\goodaction$}

With $\overlap$ and $\lkhd$, the goal agent gives a good sense of which goals are optimal at each step, but it treats both players' hands as a single object and does not reason about goals using the knowledge that each player must alternate turns. $\goodaction$ incorporates turn-taking into the goal evaluation model. As expert humans, we've found this to be the most challenging part of the game and the primary impetus for goal-changing.

I define $\goodaction$ as the following:
\begin{equation}
  \goodaction(G) = T_1 \cdot A_1(G) + T_2 \cdot A_2(G),
\label{eq:goodaction} \end{equation}
where $T_i$ and $A_i$ are defined as
\begin{align}
  T_i &= \begin{cases}
    1 & \text{currently Player $i$'s turn} \\
    0 & \text{o.w.}
  \end{cases} \\
  A_i(G) &= \begin{cases}
    1 & \exists \text{ an action Player $i$ can take that will bring them closer to $G$} \\
    0 & \text{o.w.}
  \end{cases}
\end{align}

An ``action'' is defined as a swap of 1, 2, or 3 cards between a player's hand and the table. Getting ``closer'' to a goal $G$ is equivalent to increasing the $\overlap$. Note that $\goodaction(G)$ implicitly depends on the game history $H$ in order to access information about whose turn it is and the players' hands.

\section{Evaluating Goals}

Every time the history $H$ is updated and we observe a new set of cards $C$, the weights $w_i$ for each goal $G_i$ are updated with a linear combination of the features:
\begin{equation}
  w_i := \alpha_0 \cdot \overlap(G_i,C) + \alpha_1 \cdot \lkhd(G_i,H) + \alpha_2 \cdot \goodaction(G_i)
\label{eq:w-update} \end{equation}

After the weights are updated, the model returns the ``optimal'' goal(s), which correspond to the max value(s) in $\bvec{w}$.

The values of $\{\alpha\}$ should be set to reflect the agent's preferences. A more human-like agent might weigh $\overlap$ over $\lkhd$, for example, since it tends to consider only the visible cards and does not calculate probabilities based on game history. Expert humans might also be more likely to place greater weight on $\goodaction$, which would be reflected in a high $\alpha_2$ value.

\section{Issues and Next Steps}

This goal evaluation model is rather simple, but its output at each step seems reasonable. Here are a few issues I've identified at this point:

The first issue is what values to choose for $\{\alpha\}$. Since we don't have a ``ground truth'' optimal goal baseline (which is why we're making this one!), it's difficult to evaluate the model's output in a rigorous way. Different values of $\{\alpha\}$ do noticeably affect the output, and it seems unprincipled to rely on human judgment to compare the outputs.

I am also wondering what other features we should be including. Again, I only thought of these three based on my experiences as an expert human player, but I am surely missing something that would bring the model closer to a ``ground truth''. Even if we can't create a perfect model, what heuristics should we be using to measure its performance?

\end{document}
